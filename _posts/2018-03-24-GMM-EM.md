---
layout: post
title:  "GMM and EM algorithm"
date:  2018-03-24
categories: meachine learning
tags: Meachine Learning
---

* content
{:toc}


# EM algorithm

## Definition

​	An elaborate technique of finding the maximum likelihood estimate of the parameters of a distribution from a given data set when the data is incomplete or has missing values.





## Two main applications

1. The data indeed has missing values, due to problems with or limitations of the observation process

2. The optimizing the likelihood function is analytically intractable but when the likelihood function can be simplified by assuming the existence of and values for additional but missing ( or hidden) parameters.

  ​

## EM问题抽象描述

* 我们观测到一组不完全的数据$X={x_1,x_2,…,x_N}$

* 完全的数据集为Z=(X,Y)

* 联合密度分布为:$P(Z|\Theta) = p(X,Y|\Theta)=p(Y|X,\Theta)p(X|\Theta)$

* 新的似然函数:完整数据集的似然 $L(\Theta|X,Y)=p(X,Y|\Theta)$

* $\Theta,X​$是定值,$Y​$是随机变量

* 新的似然函数为$L(\theta)=log(X|\theta)=log(\sum_Yp(X,Y|\theta))$

* 最大化上述似然函数很难,因为上述和会导致模型的参数存在耦合

* 任意的隐含变量$q(y)$分布都会定义L的一个下界,如下所示:
  $$
  L(\theta)\geq\sum_yq(y)logp(x,y|\theta)-\sum_yq(y)logq(y)=F(q,\theta)
  $$
  证明上述下界:
  $$
  \begin{align*}
  L(\theta )&=log\sum_yp(x,y|\theta)=log\sum_yq(y)\frac{p(x,y|\theta)}{q(y)} \\
  &\geq\sum_yq(y)log\frac{p(x,y|\theta)}{q(y)}[Jensen 不等式,利用log函数是凸函数的性质]\\
  &=\sum_yq(y)logp(x,y|\theta)-\sum_yq(u)logq(y)\\
  &=F(q,\theta)
  \end{align*}
  $$
  ​
